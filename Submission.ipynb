{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a7af47e",
   "metadata": {},
   "source": [
    "## All Necessary Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "842f9f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import time\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba46749f",
   "metadata": {},
   "source": [
    "### Change input path to training data accordingly "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df1beb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = r'C:\\Users\\Parthasarathy.Harini\\Downloads\\NLP\\DeepLearning\\DeepLearning_Group12\\traffic.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89c3f14f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              DateTime  Junction  Vehicles           ID\n",
      "0  2015-11-01 00:00:00         1        15  20151101001\n",
      "1  2015-11-01 01:00:00         1        13  20151101011\n",
      "2  2015-11-01 02:00:00         1        10  20151101021\n",
      "3  2015-11-01 03:00:00         1         7  20151101031\n",
      "4  2015-11-01 04:00:00         1         9  20151101041\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 48120 entries, 0 to 48119\n",
      "Data columns (total 4 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   DateTime  48120 non-null  object\n",
      " 1   Junction  48120 non-null  int64 \n",
      " 2   Vehicles  48120 non-null  int64 \n",
      " 3   ID        48120 non-null  int64 \n",
      "dtypes: int64(3), object(1)\n",
      "memory usage: 1.5+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(input_path)\n",
    "\n",
    "print(df.head())\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eec1d41",
   "metadata": {},
   "source": [
    "## Data Pre Processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b34f3268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data points per junction:\n",
      "Junction\n",
      "1    14592\n",
      "2    14592\n",
      "3    14592\n",
      "4     4344\n",
      "dtype: int64\n",
      "Training data shape: (30744, 8)\n",
      "Validation data shape: (17376, 8)\n",
      "Validation data points per junction:\n",
      "Junction\n",
      "1    4344\n",
      "2    4344\n",
      "3    4344\n",
      "4    4344\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Convert datetime to proper format\n",
    "df['DateTime'] = pd.to_datetime(df['DateTime'])\n",
    "df = df.sort_values('DateTime')\n",
    "\n",
    "# Extract time-based features\n",
    "df['hour'] = df['DateTime'].dt.hour\n",
    "df['day_of_week'] = df['DateTime'].dt.dayofweek\n",
    "df['month'] = df['DateTime'].dt.month\n",
    "df['year'] = df['DateTime'].dt.year\n",
    "\n",
    "# Check data distribution by junction\n",
    "junction_counts = df.groupby('Junction').size()\n",
    "print(\"Data points per junction:\")\n",
    "print(junction_counts)\n",
    "\n",
    "# Split data by time (2015-2016 for training, 2017 for validation)\n",
    "train_data = df[df['DateTime'].dt.year < 2017].copy()\n",
    "val_data = df[df['DateTime'].dt.year == 2017].copy()\n",
    "\n",
    "print(f\"Training data shape: {train_data.shape}\")\n",
    "print(f\"Validation data shape: {val_data.shape}\")\n",
    "\n",
    "# Check if we have validation data for each junction\n",
    "val_junction_counts = val_data.groupby('Junction').size()\n",
    "print(\"Validation data points per junction:\")\n",
    "print(val_junction_counts)\n",
    "\n",
    "# Normalize the data\n",
    "scaler = MinMaxScaler()\n",
    "df['Vehicles_scaled'] = scaler.fit_transform(df[['Vehicles']])\n",
    "train_data['Vehicles_scaled'] = scaler.transform(train_data[['Vehicles']])\n",
    "val_data['Vehicles_scaled'] = scaler.transform(val_data[['Vehicles']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa150e00",
   "metadata": {},
   "source": [
    "## Sequencing data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aae66f13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Junction 1 - Training sequences: 10224\n",
      "Junction 1 - Validation sequences: 4320\n",
      "Junction 2 - Training sequences: 10224\n",
      "Junction 2 - Validation sequences: 4320\n",
      "Junction 3 - Training sequences: 10224\n",
      "Junction 3 - Validation sequences: 4320\n",
      "Warning: Junction 4 has only 0 data points, which is not enough to create sequences of length 24\n",
      "Junction 4 - No training sequences created\n",
      "Junction 4 - Validation sequences: 4320\n",
      "X_train shape: (30672, 24)\n",
      "y_train shape: (30672,)\n",
      "X_val shape: (17280, 24)\n",
      "y_val shape: (17280,)\n"
     ]
    }
   ],
   "source": [
    "def create_sequences(data, junction_id, seq_length=24):\n",
    "   \n",
    "    junction_data = data[data['Junction'] == junction_id].copy()\n",
    "    junction_data = junction_data.sort_values('DateTime')\n",
    "\n",
    "    if len(junction_data) <= seq_length:\n",
    "        print(f\"Warning: Junction {junction_id} has only {len(junction_data)} data points, which is not enough to create sequences of length {seq_length}\")\n",
    "        return np.array([]), np.array([])\n",
    "\n",
    "    X, y = [], []\n",
    "    for i in range(len(junction_data) - seq_length):\n",
    "        X.append(junction_data['Vehicles_scaled'].iloc[i:i+seq_length].values)\n",
    "        y.append(junction_data['Vehicles_scaled'].iloc[i+seq_length])\n",
    "\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "junctions = df['Junction'].unique()\n",
    "X_train_all = []\n",
    "y_train_all = []\n",
    "X_val_all = []\n",
    "y_val_all = []\n",
    "\n",
    "for junction in junctions:\n",
    "    X_train, y_train = create_sequences(train_data, junction)\n",
    "    X_val, y_val = create_sequences(val_data, junction)\n",
    "\n",
    "    if X_train.size > 0 and y_train.size > 0:\n",
    "        X_train_all.append(X_train)\n",
    "        y_train_all.append(y_train)\n",
    "        print(f\"Junction {junction} - Training sequences: {len(X_train)}\")\n",
    "    else:\n",
    "        print(f\"Junction {junction} - No training sequences created\")\n",
    "\n",
    "    if X_val.size > 0 and y_val.size > 0:\n",
    "        X_val_all.append(X_val)\n",
    "        y_val_all.append(y_val)\n",
    "        print(f\"Junction {junction} - Validation sequences: {len(X_val)}\")\n",
    "    else:\n",
    "        print(f\"Junction {junction} - No validation sequences created\")\n",
    "\n",
    "if not X_train_all or not y_train_all or not X_val_all or not y_val_all:\n",
    "    raise ValueError(\"No sequences were created. Please check your data.\")\n",
    "X_train = np.vstack(X_train_all)\n",
    "y_train = np.concatenate(y_train_all)\n",
    "X_val = np.vstack(X_val_all)\n",
    "y_val = np.concatenate(y_val_all)\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"X_val shape: {X_val.shape}\")\n",
    "print(f\"y_val shape: {y_val.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10117687",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b32d8104",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrafficDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        X_reshaped = X.reshape(X.shape[0], X.shape[1], 1)\n",
    "        self.X = torch.tensor(X_reshaped, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32).view(-1, 1)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "train_dataset = TrafficDataset(X_train, y_train)\n",
    "val_dataset = TrafficDataset(X_val, y_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40470c0f",
   "metadata": {},
   "source": [
    "### CNN+LSTM with grid Search to optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ed256f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "318c4a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrafficCNNLSTM(nn.Module):\n",
    "    def __init__(self, seq_length, lstm_hidden_size=64, lstm_layers=2, cnn_filters=(64, 128, 128), \n",
    "                dropout_rate=0.2, kernel_size=3):\n",
    "        super(TrafficCNNLSTM, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=cnn_filters[0], kernel_size=kernel_size, padding=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=2)\n",
    "        \n",
    "        self.conv2 = nn.Conv1d(in_channels=cnn_filters[0], out_channels=cnn_filters[1], kernel_size=kernel_size, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool1d(kernel_size=2)\n",
    "        \n",
    "        self.conv3 = nn.Conv1d(in_channels=cnn_filters[1], out_channels=cnn_filters[2], kernel_size=kernel_size, padding=1)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        \n",
    "        self.cnn_output_size = cnn_filters[2]\n",
    "        self.reduced_seq_length = seq_length // 4\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.cnn_output_size,\n",
    "            hidden_size=lstm_hidden_size,\n",
    "            num_layers=lstm_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout_rate if lstm_layers > 1 else 0,\n",
    "            bidirectional=False\n",
    "        )\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc1 = nn.Linear(lstm_hidden_size, 32)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(32, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        if x.dim() == 3 and x.size(2) == 1:\n",
    "            x = x.permute(0, 2, 1)\n",
    "        elif x.dim() == 2:\n",
    "            x = x.unsqueeze(1)\n",
    "        \n",
    "        x = self.pool1(self.relu1(self.conv1(x)))\n",
    "        x = self.pool2(self.relu2(self.conv2(x)))\n",
    "        x = self.relu3(self.conv3(x))\n",
    "        \n",
    "        x = x.permute(0, 2, 1)\n",
    "        \n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        \n",
    "        lstm_out = lstm_out[:, -1, :]\n",
    "        \n",
    "        x = self.dropout(lstm_out)\n",
    "        x = self.relu4(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3766a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=50, patience=10):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    best_model_state = None\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "        \n",
    "        train_loss /= len(train_loader.dataset)\n",
    "        train_losses.append(train_loss)\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "        \n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        val_losses.append(val_loss)\n",
    "        epoch_time = time.time() - start_time\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {train_loss:.6f} | Val Loss: {val_loss:.6f} | Time: {epoch_time:.2f}s\")\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "    \n",
    "    if best_model_state:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    \n",
    "    return model, train_losses, val_losses, best_val_loss  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2111d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, val_loader, scaler):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    actual_values = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            predictions.append(outputs.cpu().numpy())\n",
    "            actual_values.append(targets.cpu().numpy())\n",
    "\n",
    "    y_pred_scaled = np.vstack(predictions)\n",
    "    y_true_scaled = np.vstack(actual_values)\n",
    "\n",
    "    y_pred = scaler.inverse_transform(y_pred_scaled)\n",
    "    y_true = scaler.inverse_transform(y_true_scaled)\n",
    "\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = math.sqrt(mse)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "    return mse, rmse, r2, y_pred, y_true\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b52272a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(train_loader, val_loader, scaler, param_grid, num_epochs=50, patience=10):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    keys = param_grid.keys()\n",
    "    values = param_grid.values()\n",
    "    hyperparameter_combinations = list(itertools.product(*values))\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    best_model = None\n",
    "    best_params = None\n",
    "    best_train_losses = None\n",
    "    best_val_losses = None\n",
    "    \n",
    "    total_combinations = len(hyperparameter_combinations)\n",
    "    for i, combination in enumerate(hyperparameter_combinations):\n",
    "        params = dict(zip(keys, combination))\n",
    "        print(f\"\\nTraining combination {i+1}/{total_combinations}:\")\n",
    "        for k, v in params.items():\n",
    "            print(f\"  {k}: {v}\")\n",
    "        \n",
    "        sample_input, _ = next(iter(train_loader))\n",
    "        seq_length = sample_input.shape[1]\n",
    "        \n",
    "        model = TrafficCNNLSTM(\n",
    "            seq_length=seq_length,\n",
    "            lstm_hidden_size=params['lstm_hidden_size'],\n",
    "            lstm_layers=params['lstm_layers'],\n",
    "            cnn_filters=params['cnn_filters'],\n",
    "            dropout_rate=params['dropout_rate'],\n",
    "            kernel_size=params['kernel_size']\n",
    "        ).to(device)\n",
    "        \n",
    "        optimizer = optim.Adam(model.parameters(), lr=params['learning_rate'])\n",
    "        criterion = nn.MSELoss()\n",
    "        \n",
    "        start_time = time.time()\n",
    "        trained_model, train_losses, val_losses, current_val_loss = train_model(\n",
    "            model, train_loader, val_loader, criterion, optimizer, \n",
    "            num_epochs=num_epochs, patience=patience\n",
    "        )\n",
    "        train_time = time.time() - start_time\n",
    "        \n",
    "        mse, rmse, r2, _, _ = evaluate_model(trained_model, val_loader, scaler)\n",
    "        \n",
    "        result = {\n",
    "            **params,\n",
    "            'val_loss': current_val_loss,\n",
    "            'mse': mse,\n",
    "            'rmse': rmse,\n",
    "            'r2': r2,\n",
    "            'train_time': train_time\n",
    "        }\n",
    "        results.append(result)\n",
    "        \n",
    "        if current_val_loss < best_val_loss:\n",
    "            best_val_loss = current_val_loss\n",
    "            best_model = trained_model\n",
    "            best_params = params\n",
    "            best_train_losses = train_losses\n",
    "            best_val_losses = val_losses\n",
    "            \n",
    "        print(f\"Results for combination {i+1}/{total_combinations}:\")\n",
    "        print(f\"  Validation Loss: {current_val_loss:.6f}\")\n",
    "        print(f\"  RMSE: {rmse:.4f}\")\n",
    "        print(f\"  R²: {r2:.4f}\")\n",
    "        print(f\"  Training Time: {train_time:.2f}s\")\n",
    "        print(\"-\" * 50)\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    results_df = results_df.sort_values('val_loss')\n",
    "    \n",
    "    print(\"\\nGrid Search Complete!\")\n",
    "    print(f\"Best hyperparameters:\")\n",
    "    for k, v in best_params.items():\n",
    "        print(f\"  {k}: {v}\")\n",
    "    print(f\"Best validation loss: {best_val_loss:.6f}\")\n",
    "    print(f\"Best RMSE: {results_df.iloc[0]['rmse']:.4f}\")\n",
    "    print(f\"Best R²: {results_df.iloc[0]['r2']:.4f}\")\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(best_train_losses, label='Training Loss')\n",
    "    plt.plot(best_val_losses, label='Validation Loss')\n",
    "    plt.title('Best Model Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    if len(results_df) > 3:\n",
    "        fig, axes = plt.subplots(3, 2, figsize=(15, 12))\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        for i, param in enumerate(param_grid.keys()):\n",
    "            if i < len(axes):\n",
    "                if param == 'cnn_filters':\n",
    "                    results_df['cnn_filters_str'] = results_df['cnn_filters'].apply(str)\n",
    "                    df_plot = results_df.groupby('cnn_filters_str')['rmse'].mean().reset_index()\n",
    "                    axes[i].bar(df_plot['cnn_filters_str'], df_plot['rmse'])\n",
    "                    axes[i].set_xticklabels(df_plot['cnn_filters_str'], rotation=90)\n",
    "                else:\n",
    "                    df_plot = results_df.groupby(param)['rmse'].mean().reset_index()\n",
    "                    axes[i].bar(df_plot[param].astype(str), df_plot['rmse'])\n",
    "                \n",
    "                axes[i].set_title(f'Effect of {param} on RMSE')\n",
    "                axes[i].set_ylabel('RMSE')\n",
    "                axes[i].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    return results_df, best_model, best_params, best_train_losses, best_val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23cf79a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "Training combination 1/96:\n",
      "  lstm_hidden_size: 32\n",
      "  lstm_layers: 1\n",
      "  cnn_filters: (32, 64, 64)\n",
      "  kernel_size: 3\n",
      "  dropout_rate: 0.2\n",
      "  learning_rate: 0.001\n",
      "Epoch 1/50 | Train Loss: 0.001931 | Val Loss: 0.001059 | Time: 5.51s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 11\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Define parameter grid\u001b[39;00m\n\u001b[0;32m      2\u001b[0m param_grid \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlstm_hidden_size\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m32\u001b[39m,\u001b[38;5;241m64\u001b[39m,\u001b[38;5;241m128\u001b[39m],\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlstm_layers\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m0.001\u001b[39m, \u001b[38;5;241m0.0005\u001b[39m]\n\u001b[0;32m      9\u001b[0m }\n\u001b[1;32m---> 11\u001b[0m results_df, best_model, best_params, best_train_losses, best_val_losses \u001b[38;5;241m=\u001b[39m \u001b[43mgrid_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_grid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\n\u001b[0;32m     13\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m mse, rmse, r2, y_pred, y_true \u001b[38;5;241m=\u001b[39m evaluate_model(best_model, val_loader, scaler)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest Model Performance:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[11], line 40\u001b[0m, in \u001b[0;36mgrid_search\u001b[1;34m(train_loader, val_loader, scaler, param_grid, num_epochs, patience)\u001b[0m\n\u001b[0;32m     37\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mMSELoss()\n\u001b[0;32m     39\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 40\u001b[0m trained_model, train_losses, val_losses, current_val_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpatience\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m train_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[0;32m     46\u001b[0m mse, rmse, r2, _, _ \u001b[38;5;241m=\u001b[39m evaluate_model(trained_model, val_loader, scaler)\n",
      "Cell \u001b[1;32mIn[9], line 29\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, val_loader, criterion, optimizer, num_epochs, patience)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs, targets \u001b[38;5;129;01min\u001b[39;00m val_loader:\n\u001b[0;32m     28\u001b[0m     inputs, targets \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), targets\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 29\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, targets)\n\u001b[0;32m     31\u001b[0m     val_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m inputs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Parthasarathy.Harini\\Downloads\\NLP\\DeepLearning\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Parthasarathy.Harini\\Downloads\\NLP\\DeepLearning\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[8], line 46\u001b[0m, in \u001b[0;36mTrafficCNNLSTM.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     43\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x)))\n\u001b[0;32m     44\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu3(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv3(x))\n\u001b[1;32m---> 46\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpermute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     48\u001b[0m lstm_out, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlstm(x)\n\u001b[0;32m     50\u001b[0m lstm_out \u001b[38;5;241m=\u001b[39m lstm_out[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'lstm_hidden_size': [32,64,128],\n",
    "    'lstm_layers': [1,2],\n",
    "    'cnn_filters': [(32,64,64),(64, 128, 128)],\n",
    "    'kernel_size': [3,5],\n",
    "    'dropout_rate': [0.2,0.3],\n",
    "    'learning_rate': [0.001, 0.0005]\n",
    "}\n",
    "\n",
    "results_df, best_model, best_params, best_train_losses, best_val_losses = grid_search(\n",
    "    train_loader, val_loader, scaler, param_grid, num_epochs=50, patience=10\n",
    ")\n",
    "mse, rmse, r2, y_pred, y_true = evaluate_model(best_model, val_loader, scaler)\n",
    "\n",
    "print(f\"Best Model Performance:\")\n",
    "print(f\"Mean Squared Error: {mse:.2f}\")\n",
    "print(f\"Root Mean Squared Error: {rmse:.2f}\")\n",
    "print(f\"R² Score: {r2:.4f}\")\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.plot(y_true[:200], label='Actual')\n",
    "plt.plot(y_pred[:200], label='Predicted')\n",
    "plt.title('Traffic Prediction: Actual vs Predicted (Best Model)')\n",
    "plt.xlabel('Time Steps')\n",
    "plt.ylabel('Number of Vehicles')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73cb172d",
   "metadata": {},
   "source": [
    "### Visualisations and saving the final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92fe8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "junction_val_indices = {}\n",
    "current_idx = 0\n",
    "\n",
    "for i, junction in enumerate(junctions):\n",
    "        junction_length = len(X_val_all[i])\n",
    "        junction_val_indices[junction] = (current_idx, current_idx + junction_length)\n",
    "        current_idx += junction_length\n",
    "if 'junction_val_indices' in locals():\n",
    "    for junction, (start_idx, end_idx) in junction_val_indices.items():\n",
    "        if end_idx > start_idx:\n",
    "            plt.figure(figsize=(15, 6))\n",
    "            plt.plot(y_true[start_idx:start_idx+200], label='Actual')\n",
    "            plt.plot(y_pred[start_idx:start_idx+200], label='Predicted')\n",
    "            plt.title(f'Traffic Prediction for Junction {junction}: Actual vs Predicted (Best Model)')\n",
    "            plt.xlabel('Time Steps')\n",
    "            plt.ylabel('Number of Vehicles')\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            plt.show()\n",
    "\n",
    "torch.save(best_model.state_dict(), 'best_traffic_cnn_lstm_model.pth')\n",
    "print(\"Best model saved successfully.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
