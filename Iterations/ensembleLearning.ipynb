{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, f1_score, mean_squared_error, r2_score\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from tqdm import tqdm  # For progress bars\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load the CSV file\n",
        "df = pd.read_csv('/content/drive/MyDrive/Deep Learning Group 12/Deep Learning Group 12/traffic.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IMXVxPL1rEAG",
        "outputId": "45720719-e246-4573-8466-16c0914fe646"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Handle datetime column\n",
        "df['datetime'] = pd.to_datetime(df.iloc[:, 0])  # First column is datetime\n",
        "df['hour'] = df['datetime'].dt.hour\n",
        "df['dayofweek'] = df['datetime'].dt.dayofweek\n",
        "df['month'] = df['datetime'].dt.month\n",
        "\n",
        "# Drop original datetime\n",
        "df = df.drop(columns=[df.columns[0], 'datetime'])\n",
        "\n",
        "# Define features and target\n",
        "X = df.iloc[:, :-1].values  # all columns except target\n",
        "y = df.iloc[:, -1].values   # target column\n",
        "\n",
        "# Print target value distribution and range\n",
        "# print(f\"Target value range: {y.min()} to {y.max()}\")\n",
        "unique_values, counts = np.unique(y, return_counts=True)\n",
        "# print(f\"Target value distribution: {len(unique_values)} unique values\")\n",
        "if len(unique_values) <= 10:\n",
        "    for value, count in zip(unique_values, counts):\n",
        "        print(f\"  Value {value}: {count} occurrences\")\n",
        "\n",
        "# Determine if we're dealing with regression or classification\n",
        "is_regression = len(unique_values) > 10\n",
        "# print(f\"Task type: {'Regression' if is_regression else 'Classification'}\")\n",
        "\n",
        "# Keep original targets for proper evaluation\n",
        "y_original = y.copy()\n",
        "\n",
        "if is_regression:\n",
        "    # Normalize to [0,1] for neural network training\n",
        "    y_normalized = (y - y.min()) / (y.max() - y.min())\n",
        "    y = y_normalized\n",
        "else:\n",
        "    # Convert to 0/1 for binary classification\n",
        "    if len(unique_values) == 2:\n",
        "        y = (y == y.max()).astype(float)\n",
        "    else:\n",
        "        pass  # Keep multi-class as is\n",
        "\n",
        "# Scale the features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_scaled, y, test_size=0.2, random_state=42\n",
        ")\n",
        "_, _, y_train_original, y_test_original = train_test_split(\n",
        "    X_scaled, y_original, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32).unsqueeze(1)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32).unsqueeze(1)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)"
      ],
      "metadata": {
        "id": "Tg4mihmfrIZ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CNN-LSTM Model Definition\n",
        "class CNNLSTMModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNNLSTMModel, self).__init__()\n",
        "        self.conv1 = nn.Conv1d(1, 32, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv1d(32, 64, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv1d(64, 64, kernel_size=3, padding=1)\n",
        "        self.lstm = nn.LSTM(input_size=64, hidden_size=64, batch_first=True)\n",
        "        self.fc1 = nn.Linear(64, 32)\n",
        "        self.fc2 = nn.Linear(32, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.conv1(x))\n",
        "        x = torch.relu(self.conv2(x))\n",
        "        x = torch.relu(self.conv3(x))\n",
        "        x = x.permute(0, 2, 1)\n",
        "        x, _ = self.lstm(x)\n",
        "        x = x[:, -1, :]  # Get the last output\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.sigmoid(self.fc2(x))\n",
        "        return x\n",
        "\n",
        "# Training function for neural networks\n",
        "def train_model(model, X_tensor, y_tensor, epochs=10, batch_size=64, lr=0.001):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.BCELoss()\n",
        "    dataset_size = len(X_tensor)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "\n",
        "        # Batch processing\n",
        "        for i in range(0, dataset_size, batch_size):\n",
        "            batch_x = X_tensor[i:i+batch_size]\n",
        "            batch_y = y_tensor[i:i+batch_size]\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(batch_x)\n",
        "            loss = criterion(outputs, batch_y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        if epoch % 2 == 0:  # Print every 2 epochs\n",
        "            epoch_loss = running_loss / ((dataset_size + batch_size - 1) // batch_size)\n",
        "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss:.4f}\")\n",
        "\n",
        "    model.eval()  # Set to evaluation mode\n",
        "    return model"
      ],
      "metadata": {
        "id": "pWN3XyvkrULc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nIMPLEMENTING BAGGING FOR CNN+LSTM\")\n",
        "n_bagging_models = 5\n",
        "bagged_models = []\n",
        "\n",
        "print(f\"Training {n_bagging_models} bagged CNN+LSTM models...\")\n",
        "for i in range(n_bagging_models):\n",
        "    print(f\"  Training bagged model {i+1}/{n_bagging_models}...\")\n",
        "\n",
        "    # Sample with replacement (bootstrap)\n",
        "    indices = np.random.choice(len(X_train), len(X_train), replace=True)\n",
        "    X_bag = X_train[indices]\n",
        "    y_bag = y_train[indices]\n",
        "\n",
        "    # Convert to tensors\n",
        "    X_bag_tensor = torch.tensor(X_bag, dtype=torch.float32).unsqueeze(1)\n",
        "    y_bag_tensor = torch.tensor(y_bag, dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "    # Create and train model\n",
        "    model = CNNLSTMModel()\n",
        "    model = train_model(model, X_bag_tensor, y_bag_tensor, epochs=5)\n",
        "    bagged_models.append(model)\n",
        "\n",
        "# Prediction function for bagged models\n",
        "def predict_bagged(X_tensor, models):\n",
        "    predictions = []\n",
        "    with torch.no_grad():\n",
        "        for model in models:\n",
        "            pred = model(X_tensor).cpu().numpy().flatten()\n",
        "            predictions.append(pred)\n",
        "\n",
        "    # Average predictions\n",
        "    ensemble_pred = np.mean(predictions, axis=0)\n",
        "    return ensemble_pred\n",
        "\n",
        "# Evaluate bagging performance\n",
        "bagged_preds = predict_bagged(X_test_tensor, bagged_models)\n",
        "\n",
        "if is_regression:\n",
        "    # For regression, scale back to original range\n",
        "    bagged_preds_orig = bagged_preds * (y_original.max() - y_original.min()) + y_original.min()\n",
        "    bagged_mse = mean_squared_error(y_test_original, bagged_preds_orig)\n",
        "    bagged_r2 = r2_score(y_test_original, bagged_preds_orig)\n",
        "    print(f\"Bagged CNN+LSTM - MSE: {bagged_mse:.4f}, RÂ²: {bagged_r2:.4f}\")\n",
        "else:\n",
        "    # For classification\n",
        "    bagged_classes = (bagged_preds > 0.5).astype(int)\n",
        "    bagged_acc = accuracy_score((y_test > 0.5).astype(int), bagged_classes)\n",
        "    bagged_f1 = f1_score((y_test > 0.5).astype(int), bagged_classes)\n",
        "    print(f\"Bagged CNN+LSTM - Acc: {bagged_acc:.4f}, F1: {bagged_f1:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "anIMFE4Arkho",
        "outputId": "daf90ab2-f314-4e50-fe7f-6865274b0b1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "IMPLEMENTING BAGGING FOR CNN+LSTM\n",
            "Training 5 bagged CNN+LSTM models...\n",
            "  Training bagged model 1/5...\n",
            "Epoch 1/5, Loss: 0.5645\n",
            "Epoch 3/5, Loss: 0.4617\n",
            "Epoch 5/5, Loss: 0.4550\n",
            "  Training bagged model 2/5...\n",
            "Epoch 1/5, Loss: 0.5759\n",
            "Epoch 3/5, Loss: 0.4643\n",
            "Epoch 5/5, Loss: 0.4570\n",
            "  Training bagged model 3/5...\n",
            "Epoch 1/5, Loss: 0.5645\n",
            "Epoch 3/5, Loss: 0.4622\n",
            "Epoch 5/5, Loss: 0.4570\n",
            "  Training bagged model 4/5...\n",
            "Epoch 1/5, Loss: 0.5494\n",
            "Epoch 3/5, Loss: 0.4652\n",
            "Epoch 5/5, Loss: 0.4593\n",
            "  Training bagged model 5/5...\n",
            "Epoch 1/5, Loss: 0.5651\n",
            "Epoch 3/5, Loss: 0.4662\n",
            "Epoch 5/5, Loss: 0.4602\n",
            "Bagged CNN+LSTM - MSE: 0.8171, RÂ²: 0.9356\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nIMPLEMENTING BOOSTING FOR CNN+LSTM\")\n",
        "n_boost_models = 3\n",
        "boosted_models = []\n",
        "sample_weights = np.ones(len(X_train)) / len(X_train)\n",
        "\n",
        "print(f\"Training {n_boost_models} boosted CNN+LSTM models...\")\n",
        "for i in range(n_boost_models):\n",
        "    print(f\"  Training boosted model {i+1}/{n_boost_models}...\")\n",
        "\n",
        "    # Sample based on weights\n",
        "    indices = np.random.choice(len(X_train), len(X_train), p=sample_weights)\n",
        "    X_boost = X_train[indices]\n",
        "    y_boost = y_train[indices]\n",
        "\n",
        "    # Convert to tensors\n",
        "    X_boost_tensor = torch.tensor(X_boost, dtype=torch.float32).unsqueeze(1)\n",
        "    y_boost_tensor = torch.tensor(y_boost, dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "    # Create and train model\n",
        "    model = CNNLSTMModel()\n",
        "    model = train_model(model, X_boost_tensor, y_boost_tensor, epochs=5)\n",
        "    boosted_models.append(model)\n",
        "\n",
        "    # Update weights based on errors (key boosting step)\n",
        "    with torch.no_grad():\n",
        "        preds = model(torch.tensor(X_train, dtype=torch.float32).unsqueeze(1)).cpu().numpy().flatten()\n",
        "        errors = np.abs(preds - y_train)\n",
        "        # Increase weights for samples with higher errors\n",
        "        sample_weights = errors / np.sum(errors)\n",
        "\n",
        "# Function to make predictions with boosted models\n",
        "def predict_boosted(X_tensor, models, weights=None):\n",
        "    if weights is None:\n",
        "        # Each subsequent model gets higher weight\n",
        "        weights = np.linspace(1, 2, len(models))\n",
        "        weights = weights / np.sum(weights)  # Normalize\n",
        "\n",
        "    predictions = []\n",
        "    with torch.no_grad():\n",
        "        for i, model in enumerate(models):\n",
        "            pred = model(X_tensor).cpu().numpy().flatten()\n",
        "            predictions.append(pred * weights[i])\n",
        "\n",
        "    # Weighted sum of predictions\n",
        "    ensemble_pred = np.sum(predictions, axis=0)\n",
        "    return ensemble_pred\n",
        "\n",
        "# Evaluate boosting performance\n",
        "boosted_preds = predict_boosted(X_test_tensor, boosted_models)\n",
        "\n",
        "if is_regression:\n",
        "    # For regression, scale back to original range\n",
        "    boosted_preds_orig = boosted_preds * (y_original.max() - y_original.min()) + y_original.min()\n",
        "    boosted_mse = mean_squared_error(y_test_original, boosted_preds_orig)\n",
        "    boosted_r2 = r2_score(y_test_original, boosted_preds_orig)\n",
        "    print(f\"Boosted CNN+LSTM - MSE: {boosted_mse:.4f}, RÂ²: {boosted_r2:.4f}\")\n",
        "else:\n",
        "    # For classification\n",
        "    boosted_classes = (boosted_preds > 0.5).astype(int)\n",
        "    boosted_acc = accuracy_score((y_test > 0.5).astype(int), boosted_classes)\n",
        "    boosted_f1 = f1_score((y_test > 0.5).astype(int), boosted_classes)\n",
        "    print(f\"Boosted CNN+LSTM - Acc: {boosted_acc:.4f}, F1: {boosted_f1:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D-hohvOtmk8T",
        "outputId": "571820ea-4496-4391-9ef6-16848b7c545f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "IMPLEMENTING BOOSTING FOR CNN+LSTM\n",
            "Training 3 boosted CNN+LSTM models...\n",
            "  Training boosted model 1/3...\n",
            "Epoch 1/5, Loss: 0.5585\n",
            "Epoch 3/5, Loss: 0.4626\n",
            "Epoch 5/5, Loss: 0.4533\n",
            "  Training boosted model 2/3...\n",
            "Epoch 1/5, Loss: 0.5614\n",
            "Epoch 3/5, Loss: 0.4835\n",
            "Epoch 5/5, Loss: 0.4705\n",
            "  Training boosted model 3/3...\n",
            "Epoch 1/5, Loss: 0.5801\n",
            "Epoch 3/5, Loss: 0.4946\n",
            "Epoch 5/5, Loss: 0.4747\n",
            "Boosted CNN+LSTM - MSE: 0.3275, RÂ²: 0.9742\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nSUMMARY OF ENSEMBLE METHODS\")\n",
        "# Display comparison of bagging and boosting methods\n",
        "if is_regression:\n",
        "    print(\"\\nRegression Performance Metrics (MSE, RÂ²):\")\n",
        "    print(f\"Bagged CNN+LSTM:    MSE={bagged_mse:.4f}, RÂ²={bagged_r2:.4f}\")\n",
        "    print(f\"Boosted CNN+LSTM:   MSE={boosted_mse:.4f}, RÂ²={boosted_r2:.4f}\")\n",
        "else:\n",
        "    print(\"\\nClassification Performance Metrics (Accuracy, F1):\")\n",
        "    print(f\"Bagged CNN+LSTM:    Acc={bagged_acc:.4f}, F1={bagged_f1:.4f}\")\n",
        "    print(f\"Boosted CNN+LSTM:   Acc={boosted_acc:.4f}, F1={boosted_f1:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uHWGmyhHrZRp",
        "outputId": "a25e82bb-4ba7-4a52-ca9c-539142310fc8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "SUMMARY OF ENSEMBLE METHODS\n",
            "\n",
            "Regression Performance Metrics (MSE, RÂ²):\n",
            "Bagged CNN+LSTM:    MSE=0.8171, RÂ²=0.9356\n",
            "Boosted CNN+LSTM:   MSE=0.3275, RÂ²=0.9742\n"
          ]
        }
      ]
    }
  ]
}